{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9178898,"sourceType":"datasetVersion","datasetId":5547637},{"sourceId":9456265,"sourceType":"datasetVersion","datasetId":5748444},{"sourceId":9483694,"sourceType":"datasetVersion","datasetId":5768979}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.transforms.functional as ft\nfrom torch.utils.data import Dataset\n!pip install -U albumentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T06:11:50.156021Z","iopub.execute_input":"2024-09-26T06:11:50.156987Z","iopub.status.idle":"2024-09-26T06:12:10.996785Z","shell.execute_reply.started":"2024-09-26T06:11:50.156928Z","shell.execute_reply":"2024-09-26T06:12:10.995788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:12:17.647253Z","iopub.execute_input":"2024-09-26T06:12:17.647761Z","iopub.status.idle":"2024-09-26T06:12:18.703438Z","shell.execute_reply.started":"2024-09-26T06:12:17.647724Z","shell.execute_reply":"2024-09-26T06:12:18.702495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNET(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features= [64, 128, 256, 512]):\n        super(UNET, self).__init__()\n\n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Down-sampling\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n\n        # Up-sampling\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(\n                    feature*2, feature, kernel_size=2, stride=2\n                )\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n\n        self.bottom = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        x = self.bottom(x)\n\n        skip_connections = skip_connections[::-1]\n\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx // 2]\n\n            if x.shape != skip_connection.shape:  # Required if we have not chosen the input and output size\n                # according to MaxPool2d\n                x = ft.resize(x, size=(skip_connection.shape[2:]))\n            concat_skip = torch.concat((skip_connection, x), dim=1)\n\n            x = self.ups[idx+1](concat_skip)\n\n        return self.final_conv(x)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:25:53.251728Z","iopub.execute_input":"2024-09-26T06:25:53.252143Z","iopub.status.idle":"2024-09-26T06:25:53.267412Z","shell.execute_reply.started":"2024-09-26T06:25:53.252105Z","shell.execute_reply":"2024-09-26T06:25:53.266413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Dataset","metadata":{}},{"cell_type":"code","source":"def random_splits(prob, image_list):\n    img_lis_1 = random.sample(image_list, int(prob*len(image_list)))\n    img_lis_2 = []\n    for img in image_list:\n        if img not in img_lis_1:\n            img_lis_2.append(img)\n            \n    return img_lis_1, img_lis_2","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:26:00.504336Z","iopub.execute_input":"2024-09-26T06:26:00.505244Z","iopub.status.idle":"2024-09-26T06:26:00.510512Z","shell.execute_reply.started":"2024-09-26T06:26:00.505203Z","shell.execute_reply":"2024-09-26T06:26:00.509485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TreeSegmentDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, images, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.images = images\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.image_dir, self.images[index])\n        mask_path = os.path.join(self.mask_dir, self.images[index].replace('.jpg', '.png'))\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32) / 255\n        \n        if self.transform:\n            augmentations = self.transform(image=image, mask=mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n\n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:26:02.561503Z","iopub.execute_input":"2024-09-26T06:26:02.562269Z","iopub.status.idle":"2024-09-26T06:26:02.570570Z","shell.execute_reply.started":"2024-09-26T06:26:02.562231Z","shell.execute_reply":"2024-09-26T06:26:02.569347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BeachDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, images, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transforms = transform\n        self.images = images\n        # self.images.remove(\".DS_Store\")\n        # self.masks.remove(\".DS_Store\")\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.image_dir, self.images[index])\n        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\"image\", \"mask\"))\n\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32) / 255\n\n        if self.transforms:\n            augmentations = self.transforms(image=image, mask=mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n\n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:26:05.341933Z","iopub.execute_input":"2024-09-26T06:26:05.342806Z","iopub.status.idle":"2024-09-26T06:26:05.350658Z","shell.execute_reply.started":"2024-09-26T06:26:05.342766Z","shell.execute_reply":"2024-09-26T06:26:05.349727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"code","source":"LEARNING_RATE = 1e-4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nNUM_EPOCHS = 50\nNUM_WORKERS = 4\nIMAGE_HEIGHT = 512\nIMAGE_WIDTH = 512\nPIN_MEMORY = True\nLOAD_MODEL = True\nTREE_IMAGE_DIR= \"/kaggle/input/tree-binary-segmentation/images/images\"\nTREE_MASK_DIR = \"/kaggle/input/tree-binary-segmentation/masks/masks\"\nCOAST_IMAGE_DIR = \"/kaggle/input/beach-segmentation/seg_data_new/images\"\nCOAST_MASK_DIR = \"/kaggle/input/beach-segmentation/seg_data_new/masks\"\n# PRE_TRAIN_IMAGES, PRE_VAL_IMAGES = random_splits(0.7, os.listdir(TREE_IMAGE_DIR))\nTRAIN_IMAGES, VAL_IMAGES = random_splits(0.7, os.listdir(COAST_IMAGE_DIR))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:26:28.441480Z","iopub.execute_input":"2024-09-26T06:26:28.441846Z","iopub.status.idle":"2024-09-26T06:26:28.505793Z","shell.execute_reply.started":"2024-09-26T06:26:28.441813Z","shell.execute_reply":"2024-09-26T06:26:28.505008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions and Transforms","metadata":{}},{"cell_type":"code","source":"def train_fn(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader)\n\n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(device=DEVICE)\n        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n\n        # Forward\n        with torch.amp.autocast('cuda'):\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n\n        # Backward\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n\n        # Updating tqdm loop\n        loop.set_postfix(loss=loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:26:31.744264Z","iopub.execute_input":"2024-09-26T06:26:31.744638Z","iopub.status.idle":"2024-09-26T06:26:31.751799Z","shell.execute_reply.started":"2024-09-26T06:26:31.744604Z","shell.execute_reply":"2024-09-26T06:26:31.750863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transforms = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=90, p=1.0),\n        A.HorizontalFlip(p=0.8),\n        A.VerticalFlip(p=0.8),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2()\n    ],\n)\n\nval_transforms = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:26:32.922944Z","iopub.execute_input":"2024-09-26T06:26:32.923813Z","iopub.status.idle":"2024-09-26T06:26:32.932677Z","shell.execute_reply.started":"2024-09-26T06:26:32.923773Z","shell.execute_reply":"2024-09-26T06:26:32.931654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving Checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model: torch.nn.Module):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n\n\ndef get_loaders(\n        img_dir,\n        mask_dir,\n        train_images,\n        val_images,\n        batch_size,\n        train_transform,\n        val_transform,\n        num_workers,\n        pin_memory=True,\n        beach_dataset:bool = False\n):\n    if beach_dataset:\n        train_ds = BeachDataset(image_dir=img_dir,\n                                mask_dir=mask_dir,\n                                images = train_images,\n                                transform=train_transform)\n        \n        val_ds = BeachDataset(image_dir=img_dir,\n                              mask_dir=mask_dir,\n                              images = val_images,\n                              transform=val_transform)\n    else:\n        train_ds = TreeSegmentDataset(image_dir=img_dir,\n                                      mask_dir=mask_dir,\n                                      images = train_images,\n                                      transform=train_transform)\n\n\n        val_ds = TreeSegmentDataset(image_dir=img_dir,\n                                    mask_dir=mask_dir,\n                                    images = val_images,\n                                    transform=val_transform)\n    \n    \n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=True\n    )\n\n    \n\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=False\n    )\n\n    return train_loader, val_loader\n\n\ndef check_accuracy(loader, model, device, acc_list, dice_list):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n\n    with torch.inference_mode():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2*(preds * y).sum()) / ((preds + y).sum() + 1e-8)\n    \n    \n    print(\n        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.4f}\"\n    )\n\n    print(f\"Dice score: {dice_score/len(loader)}\")\n    acc = round(((num_correct*100)/num_pixels).item(), 4)\n    acc_list.append(acc)\n    dice_list.append((dice_score/ len(loader)).item())\n    model.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:26:46.609147Z","iopub.execute_input":"2024-09-26T06:26:46.609544Z","iopub.status.idle":"2024-09-26T06:26:46.624004Z","shell.execute_reply.started":"2024-09-26T06:26:46.609504Z","shell.execute_reply":"2024-09-26T06:26:46.622954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_weight = torch.tensor(59.6704)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:26:53.452936Z","iopub.execute_input":"2024-09-26T06:26:53.453812Z","iopub.status.idle":"2024-09-26T06:26:53.472249Z","shell.execute_reply.started":"2024-09-26T06:26:53.453771Z","shell.execute_reply":"2024-09-26T06:26:53.471290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_weight.item()","metadata":{"execution":{"iopub.status.busy":"2024-09-25T21:18:07.312823Z","iopub.execute_input":"2024-09-25T21:18:07.313215Z","iopub.status.idle":"2024-09-25T21:18:07.319323Z","shell.execute_reply.started":"2024-09-25T21:18:07.313180Z","shell.execute_reply":"2024-09-25T21:18:07.318339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting-up model, dataloaders, loss-function, optimizer, learning rate scheduler","metadata":{}},{"cell_type":"code","source":"model = UNET(in_channels=3, out_channels=1)\n# model = nn.DataParallel(model)\nmodel.to(device=DEVICE)\nloss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, [15, 30], gamma=0.1)\ntrain_loader, val_loader = get_loaders(\n    COAST_IMAGE_DIR,\n    COAST_MASK_DIR,\n    TRAIN_IMAGES,\n    VAL_IMAGES,\n    BATCH_SIZE,\n    train_transforms,\n    val_transforms,\n    num_workers=NUM_WORKERS,\n    beach_dataset = True\n)\n\nif LOAD_MODEL:\n    load_checkpoint(torch.load(\"/kaggle/input/tree-segment-checkpoint/treesegmentpre-train.tar\"), model)\nscaler = torch.amp.GradScaler('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:27:44.856070Z","iopub.execute_input":"2024-09-26T06:27:44.857139Z","iopub.status.idle":"2024-09-26T06:27:48.048631Z","shell.execute_reply.started":"2024-09-26T06:27:44.857087Z","shell.execute_reply":"2024-09-26T06:27:48.047668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"acc_list, dice_list = [], []\ncheck_accuracy(val_loader, model, device=DEVICE, acc_list=acc_list, dice_list=dice_list)\nfor epoch in range(NUM_EPOCHS):\n    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n    scheduler.step()\n    if epoch == 49:\n        checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict()\n        }\n        save_checkpoint(checkpoint)\n    check_accuracy(val_loader, model, device=DEVICE, acc_list=acc_list, dice_list=dice_list)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:28:02.178878Z","iopub.execute_input":"2024-09-26T06:28:02.179536Z","iopub.status.idle":"2024-09-26T06:45:49.940003Z","shell.execute_reply.started":"2024-09-26T06:28:02.179494Z","shell.execute_reply":"2024-09-26T06:45:49.938784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10), dpi=200)\nax.plot(range(len(dice_list)), dice_list, c='black')\nax.set_xlabel(\"Epochs\")\nax.set_ylabel(\"Dice Score\")\nplt.savefig('dice.png')","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:52:00.423771Z","iopub.execute_input":"2024-09-26T06:52:00.424228Z","iopub.status.idle":"2024-09-26T06:52:01.193164Z","shell.execute_reply.started":"2024-09-26T06:52:00.424185Z","shell.execute_reply":"2024-09-26T06:52:01.191967Z"},"trusted":true},"execution_count":null,"outputs":[]}]}